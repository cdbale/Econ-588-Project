---
title: "588 Final Project"
author: "Cameron Bale"
date: "March 27, 2019"
output: github_document
---

I plan to model the number of units of cereal sold based on several independent variables. I will examine the ability
to make inference using the model based on different implementations of standard errors. The data is at the level of,
"How many units of a given upc were sold in a given store in a given week?"

***

Install and load packages. Increase memory limit.
```{r}
#install.packages('tidyverse')
#install.packages('quantreg')
library(tidyverse)
library(quantreg)
library(lubridate)
memory.limit(size = 50000)
```

# Reading in the Data

Create a loop that:

* reads in the data for a given year (2010 - 2013)
* combines the movement data with the store file for the year
* filters for grocery stores in Utah
* joins with the upc file
* saves the full data file
```{r}
products <- read_tsv('products.tsv', quote = "")

years <- c(10, 11, 12, 13)

for (i in seq_along(years)) {

  move <- read_tsv(paste0('1344_20', years[i], '.tsv'))
  stores <- read_tsv(paste0('stores_20', years[i], '.tsv'), guess_max = 200000)

  full <- move %>%
    inner_join(stores, by = 'store_code_uc') %>%
    filter(channel_code == 'F' & fips_state_descr == 'UT') %>%
    inner_join(products, by = 'upc')

  save(full, file = paste0('full_', years[i], '.RData'))

  rm(move, stores, full)

}
```

Load data for all four years and combine.
```{r}
full_data <- tibble()

for (i in seq_along(years)) {
  
  load(paste0('full_', years[i], '.RData'))
  
  full_data <- full_data %>%
    bind_rows(full)
  
}

rm(full)
```

# Data Cleaning

View variable types.
```{r}
str(full_data)
```

Coercing upc and store zip code to proper data types.
```{r}
full_data$upc <- as.numeric(full_data$upc)
full_data$store_zip3 <- as.numeric(full_data$store_zip3)
```

View summary of continuous, independent variables.
```{r}
full_data %>%
  select(price, units) %>%
  summary()
```
The price variable has some outliers in the max. No box of cereal costs 279, I'm assuming it must be a case that 
was purchased, but I'm going to drop anything over 12 dollars for this analysis. The units variable is probably skewed
right.

Calculate 0.0001 and 0.9999 quantiles of data to use to remove outliers.
```{r}
price_quantiles <- quantile(full_data$price, probs = c(0.0001, 0.9999))
unit_quantiles <- quantile(full_data$units, probs = c(0.0001, 0.9999))
```

Filter for units contained within the 1st and 99th quantiles of price and units sold.
```{r}
full_data <- full_data %>%
  filter(price > price_quantiles[1] & price < price_quantiles[2],
         units > unit_quantiles[1] & units < unit_quantiles[2])
```

Convert week_end into date format, and remove observations that have missing values.
```{r}
full_data <- full_data %>%
  mutate(week_end = ymd(week_end))
```

View histogram of units sold.
```{r}
full_data %>%
  ggplot(aes(x = units)) +
  geom_histogram()
```
This seems pretty skewed so lets transform it using a natural log.

View histogram of natural log of units sold.
```{r}
full_data %>%
  ggplot(aes(x = log(units))) +
  geom_histogram()
```
This looks a bit better.

View histogram of price.
```{r}
full_data %>%
  ggplot(aes(x = price)) +
  geom_histogram()
```
This looks pretty good.

Let's create a log version of units sold.
```{r}
full_data <- full_data %>%
  mutate(lunits = log(units)) %>%
  mutate(size = size1_amount)
```

Load in google trends search data for keyword 'cereal'.
```{r}
trends <- read_csv('multiTimeline.csv') %>%
  mutate(week_end = mdy(week_end) + 6)
```

Filter the trend data so that dates line up.
```{r}
trends <- trends %>%
  filter(week_end < '2014-01-04' & week_end > '2009-12-26')
```

Join trend data to full data.
```{r}
full_data <- full_data %>%
  left_join(trends, by = 'week_end')
```

Save data for future use.
```{r}
save(full_data, file = 'full_data.RData')
```

Load data.
```{r}
#load('full_data.RData')
```

# Model

I want to include the following variables to explain the number of units sold;

* **price** : what price was the product sold at.
* **feature** : All retailer advertisements found in local newspapers, free standing inserts (FSIs), and
free standing circulars, and may also include online ads from the retailer's website. The vast
majority of featured items will include a price discount, but they don't have to. Features include
Major Ads (which typically include an image as well as the price of an item), Line Ads (only has
the name and price of the item), and retailer coupons that can be redeemed at the register.
* **display** : Display - a secondary location of an item in the store that is non-permanent and intended for
merchandising purposes. Displays are located in the Store Lobby, Front of Store, End of Aisle, In
Aisle, or Back of Store. Displayed items may or may not have an associated price decrease.
* **size** : size in oz of product

and depending on the model, I'll include fixed effects/clustering/random effects, etc. (tbd) for the following:

* **store_code_uc** : unique store identifier
* **week_end** : control for seasonality using google trends? identifies the date
* **year** : gives the year
* **retailer_code** : unique retailer identifier
* **store_zip** : identifies the first three digits of store zip code
* **fips_county** : identifies the county a store is located in

```{r}
full_data <- full_data %>%
  select(upc, 
         store_code_uc, 
         retailer_code, 
         store_zip3, 
         fips_county_descr, 
         week_end, 
         year,
         units,
         lunits,
         price, 
         size, 
         feature, 
         display,
         search_frequency) %>%
  drop_na()
```

Visualize data.
```{r}
full_data %>%
  ggplot(aes(x = price, y = lunits)) +
  geom_point() +
  labs(x = 'Price',
       y = 'Log Units',
       title = 'Natural Log of Units Sold Against Price')
```

Run model.
```{r}
(mdl_sum <- full_data %>%
  lm(lunits ~ price + size + feature + display + search_frequency, data = .) %>%
  summary())
```

Converting coefficients to be intrepreted as, for a 1 unit increase in the independent variable, the the number of
units sold changes by __ percent.
```{r}
(mdl_sum$coefficients[,1] <- (exp(mdl_sum$coefficients[,1]) - 1) * 100)
```

Examining proxy variable. We want it to be correlated with a variable that is difficult or impossible to measure.
In this case, the google search frequency is a proxy for seasonality.
```{r}
cor(full_data$lunits, full_data$search_frequency)
```

# Ice Cream

Read in files for 2011 (and products file). Merge the data from the 'stores' file and the 'products' master file to the movement file. Filter for channel code 'F', which denotes 'Food' and 
for the 4 states where supervalue operates (North Dakota, Minnesota, Missouri, and D.C.).

```{r}
untar("ice_cream.tgz")

move_11<-read_tsv("nielsen_extracts/RMS/2011/Movement_Files/2005_2011/2672_2011.tsv")
stores_11<-read_tsv("nielsen_extracts/RMS/2011/Annual_Files/stores_2011.tsv")
products<-read_tsv("nielsen_extracts/RMS/Master_Files/Latest/products.tsv", quote="")

full_11 <- move_11 %>%
  inner_join(stores_11, by ="store_code_uc") %>% 
  filter(channel_code == "F" & fips_state_descr == 'UT') %>%
  inner_join(products, by ="upc")

rm(move_11, stores_11)
```

Read in files for 2012. Merge the data from the 'stores' file and the 'products' master file to the movement file. Filter for channel code 'F', which denotes 'Food' and 
for the 4 states where supervalue operates (North Dakota, Minnesota, Missouri, and D.C.).Remove move/stores files from 2011 and 2012 for memory space. 

```{r}
move_12<-read_tsv("nielsen_extracts/RMS/2012/Movement_Files/2005_2012/2672_2012.tsv")
stores_12<-read_tsv("nielsen_extracts/RMS/2012/Annual_Files/stores_2012.tsv")

full_12 <- move_12 %>%
  inner_join(stores_12, by ="store_code_uc") %>% 
  filter(channel_code == "F" & fips_state_descr == 'UT') %>%
  inner_join(products, by ="upc")

rm(move_12, stores_12)
```

Read in files for 2013. Merge the data from the 'stores' file and the 'products' master file to the movement file. Filter for channel code 'F', which denotes 'Food' and 
for the 4 states where supervalue operates (North Dakota, Minnesota, Missouri, and D.C.).

```{r}
move_13<-read_tsv("nielsen_extracts/RMS/2013/Movement_Files/2005_2013/2672_2013.tsv")
stores_13<-read_tsv("nielsen_extracts/RMS/2013/Annual_Files/stores_2013.tsv", guess_max = 200000)

full_13 <- move_13 %>%
  inner_join(stores_13, by ="store_code_uc") %>% 
  filter(channel_code == "F" & fips_state_descr == 'UT') %>%
  inner_join(products, by ="upc")
```

Combine 'full_11', 'full_12', and 'full_13' into one file, 'full_11_12_13' containing the data for all years.
Change 'week_end' variable to year-month-day format.

```{r}
full_data <- full_11 %>%
  bind_rows(full_12) %>%
  bind_rows(full_13) %>%
  mutate(week_end = ymd(week_end))
```

Read in ice cream trends data.
```{r}
trends <- read_csv('ice_cream_trends.csv') %>%
  mutate(week_end = mdy(week_end) + 6)
```

Add google search trends to full data.
```{r}
full_data <- full_data %>%
  left_join(trends, by = 'week_end')
```

Plot search frequency against units sold.
```{r}
full_data %>%
  group_by(week_end) %>%
  summarize(sales = sum(units * price), level_of_interest = mean(level_of_interest)) %>%
  gather(key = which_line, value = amount, -week_end) %>%
  ggplot(aes(x = week_end, y = log(amount), color = which_line)) +
  geom_line()
```

```{r}
cor(full_data$units, full_data$level_of_interest)
```













